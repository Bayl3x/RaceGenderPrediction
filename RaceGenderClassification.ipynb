{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps and Explainations:\n",
    "Hi all, below are my explaination of the process I took to build this model and the issues I encountered. I will include training and testing stats in the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from keras.engine import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, Activation, Dropout, Flatten, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0:\n",
    "Since both race and gender model is based on a pre-trained model, I created a function to select which model to train and to define the apporiate training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, sample_size_per_group, split, epoch, num_of_batches, epoch_patience):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "This step extracts data from the images provided. The images are resized, converted to a vector, and stored in a list. Two additional lists are created to include the labels for the images based on the folder where they reside.\n",
    "    \n",
    "Now due to the limitation of my machines at home, I have to down sample the dataset because my machine either takes too long to train or will eventually run out of memory. To do this, I randomly select a subset of images from each group to be imported. This also addresses the issue of unbalanced classes because each class will now contain the same number of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # extract data from image\n",
    "    print('Extracting data from images...')\n",
    "    \n",
    "    X = []\n",
    "    y_race = []\n",
    "    y_gender = []\n",
    "    \n",
    "    image_files = []\n",
    "    image_sample = []\n",
    "    \n",
    "    race = ['asian', 'black', 'indian', 'latino', 'white']\n",
    "    gender = ['man', 'woman']\n",
    "    \n",
    "    image_files = os.listdir('images/asianman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/asianman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('man')\n",
    "            y_race.append('asian')\n",
    "    \n",
    "    image_files = os.listdir('images/asianwoman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/asianwoman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('woman')\n",
    "            y_race.append('asian')\n",
    "    \n",
    "    image_files = os.listdir('images/blackman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/blackman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('man')\n",
    "            y_race.append('black')\n",
    "    \n",
    "    image_files = os.listdir('images/blackwoman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/blackwoman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('woman')\n",
    "            y_race.append('black')\n",
    "            \n",
    "    image_files = os.listdir('images/indianman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/indianman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('man')\n",
    "            y_race.append('indian')\n",
    "                \n",
    "    image_files = os.listdir('images/indianwoman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/indianwoman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('woman')\n",
    "            y_race.append('indian')\n",
    "          \n",
    "    image_files = os.listdir('images/latinoman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/latinoman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('man')\n",
    "            y_race.append('latino')\n",
    "    \n",
    "    image_files = os.listdir('images/latinowoman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/latinowoman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image) \n",
    "            X.append(image)\n",
    "            y_gender.append('woman')\n",
    "            y_race.append('latino')\n",
    "         \n",
    "    image_files = os.listdir('images/whiteman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/whiteman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)     \n",
    "            X.append(image)\n",
    "            y_gender.append('man')\n",
    "            y_race.append('white')\n",
    "          \n",
    "    image_files = os.listdir('images/whitewoman')\n",
    "    image_sample = random.sample(image_files, sample_size_per_group)\n",
    "    for filename in image_sample:\n",
    "        if 'jpg' in filename:\n",
    "            image = cv2.imread('images/whitewoman/' + filename)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = img_to_array(image)\n",
    "            X.append(image)\n",
    "            y_gender.append('woman')\n",
    "            y_race.append('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "This is a standard step where I normalize the data, encode the labels, transform them to vectors and split them into train/test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # prepare the data\n",
    "    print('Processing data...')\n",
    "    \n",
    "    X = np.array(X, dtype=\"float\") / 255.0\n",
    "    y_gender = np.array(y_gender)\n",
    "    y_race = np.array(y_race)\n",
    "    \n",
    "    enc_race = preprocessing.LabelEncoder()\n",
    "    enc_gender = preprocessing.LabelEncoder()\n",
    "    \n",
    "    enc_race.fit(race)\n",
    "    enc_gender.fit(gender)\n",
    "    \n",
    "    y_race = enc_race.transform(y_race)\n",
    "    y_gender = enc_gender.transform(y_gender)\n",
    "    \n",
    "    y_race = to_categorical(y_race, num_classes = len(race))\n",
    "    y_gender = to_categorical(y_gender, num_classes = len(gender))\n",
    "    \n",
    "    X_gender_train, X_gender_test, y_gender_train, y_gender_test = train_test_split(X, y_gender, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "    X_race_train, X_race_test, y_race_train, y_race_test = train_test_split(X, y_race, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "Here I load a pre-trained face detection mode and its weights. This model is used for face recognition and feature extraction. These layers are locked and will not be train by the models below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # build model\n",
    "    print('Loading face model...')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "     \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "     \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "     \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "     \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "     \n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.load_weights('vgg_face_weights.h5')\n",
    "    \n",
    "    kfold = KFold(n_splits = split, shuffle = True)\n",
    "    early_stop = EarlyStopping(monitor = 'loss', patience = epoch_patience)\n",
    "    \n",
    "    for layer in model.layers[:-7]:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "Here I build and train the model for gender detection. The training uses a kfold cross validation and the model with the best evaluation accuracy is selected as the final model. Due to the limitation of my machine the number of folder, epoch, and batch size are limited to a small number (see build_model.py for parameters). An early stopping is used to prevent overfitting, it does so by monitoring the training loss and stoping after 3 epochs of unimproved loss. The stats below showed good resuslts for this model as shown in the confusion matrix below. However The training loss stop decreasing after some epochs and started increasing, this is likely due to the learning rate being too large at this point and causing the gradient to not converge. Vanishing/Exploding Gradient can also cause this issue but will need more epochs to observe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # gender model\n",
    "    if  model_type == 'gender':\n",
    "        print('Building gender model...')\n",
    "        i = 0\n",
    "        gender_model = [None] * split\n",
    "        gender_model_train = [None] * split\n",
    "        gender_model_evaluate = [None] * split\n",
    "        \n",
    "        for train, test in kfold.split(X_gender_train, y_gender_train):\n",
    "            print('Training model ' + str(i) + '...')\n",
    "            gender_model_output = Sequential()\n",
    "            gender_model_output = Convolution2D(len(gender), (1, 1), name = 'predictions')(model.layers[-4].output)\n",
    "            gender_model_output = Flatten()(gender_model_output)\n",
    "            gender_model_output = Activation('softmax')(gender_model_output)\n",
    "            \n",
    "            gender_model[i] = Model(inputs = model.input, outputs = gender_model_output)\n",
    "            \n",
    "            gender_model[i].compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])\n",
    "            \n",
    "            gender_model_train[i] = gender_model[i].fit(X[train], y_gender[train],\n",
    "                                                  validation_data = (X_gender_train[test], y_gender_train[test]),\n",
    "                                                  batch_size = len(X_gender_train[train])//num_of_batches,\n",
    "                                                  epochs = epoch, callbacks = [early_stop], verbose = 1)\n",
    "            \n",
    "            print('Evaluating model ' + str(i) + '...')\n",
    "            gender_model_evaluate[i] = gender_model[i].evaluate(X_gender_test, y_gender_test, batch_size = len(X_gender_train[train])//num_of_batches)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        best_model = 0\n",
    "        for m in range (len(gender_model_evaluate) - 1):\n",
    "            if gender_model_evaluate[m][1] > gender_model_evaluate[m + 1][1]:\n",
    "                best_model = m\n",
    "             \n",
    "        gender_model[m].save('gender_model.h5')\n",
    "        gender_model[m].save_weights(\"gender_model_weights.h5\")\n",
    "        \n",
    "        plt.title('Gender Model Loss')\n",
    "        plt.plot(gender_model_train[0].history['loss'], label = 'train')\n",
    "        plt.plot(gender_model_train[0].history['val_loss'], label = 'val')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig('gender_model_loss.png')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title('Gender Model Accuracy')\n",
    "        plt.plot(gender_model_train[0].history['accuracy'], label = 'train')\n",
    "        plt.plot(gender_model_train[0].history['val_accuracy'], label = 'val')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig('gender_model_accuracy.png')\n",
    "        plt.show()\n",
    "        \n",
    "        np.savetxt('gender_model_evaluation.txt', gender_model_evaluate[0])\n",
    "        print ('Test Loss, Test Accuracy:', gender_model_evaluate[0])\n",
    "        \n",
    "        gender_predicted = gender_model[0].predict(X_gender_test)\n",
    "        gender_predicted_class = []\n",
    "        gender_actual_class = []\n",
    "        \n",
    "        for p in range (0, len(gender_predicted)):\n",
    "            gender_predicted_class.append(gender[np.argmax(gender_predicted[p])])\n",
    "            gender_actual_class.append(gender[np.argmax(y_gender_test[p])])\n",
    "        \n",
    "        cm = confusion_matrix(gender_actual_class, gender_predicted_class, gender)\n",
    "        \n",
    "        df_cm = pd.DataFrame(cm, index = gender, columns = gender)\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.title('Gender Confusion Matrix')\n",
    "        plt.ylabel('True')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.savefig('gender_model_cm.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training history from the best model...\n",
    "Extracting data from images...\n",
    "Processing data...\n",
    "Loading face model...\n",
    "Building gender model...\n",
    "Training model 0...\n",
    "Train on 1000 samples, validate on 500 samples\n",
    "Epoch 1/10\n",
    "1000/1000 [==============================] - 488s 488ms/step - loss: 0.6629 - accuracy: 0.7290 - val_loss: 0.2740 - val_accuracy: 0.8800\n",
    "Epoch 2/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.1682 - accuracy: 0.9450 - val_loss: 0.1706 - val_accuracy: 0.9640\n",
    "Epoch 3/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.1009 - accuracy: 0.9750 - val_loss: 0.1114 - val_accuracy: 0.9660\n",
    "Epoch 4/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0532 - accuracy: 0.9870 - val_loss: 0.1135 - val_accuracy: 0.9600\n",
    "Epoch 5/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0150 - accuracy: 0.9930 - val_loss: 0.1441 - val_accuracy: 0.9560\n",
    "Epoch 6/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 0.1410 - val_accuracy: 0.9640\n",
    "Epoch 7/10\n",
    "1000/1000 [==============================] - 482s 482ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.1270 - val_accuracy: 0.9700\n",
    "Epoch 8/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.1220 - val_accuracy: 0.9780\n",
    "Epoch 9/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 0.1230 - val_accuracy: 0.9800\n",
    "Epoch 10/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0126 - accuracy: 0.9950 - val_loss: 0.1425 - val_accuracy: 0.9720\n",
    "Evaluating model 0...\n",
    "500/500 [==============================] - 93s 186ms/step\n",
    "Training model 1...\n",
    "Train on 1000 samples, validate on 500 samples\n",
    "Epoch 1/10\n",
    "1000/1000 [==============================] - 483s 483ms/step - loss: 0.2041 - accuracy: 0.9270 - val_loss: 0.1385 - val_accuracy: 0.9720\n",
    "Epoch 2/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.1357 - accuracy: 0.9690 - val_loss: 0.1645 - val_accuracy: 0.9640\n",
    "Epoch 3/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0887 - accuracy: 0.9790 - val_loss: 0.0819 - val_accuracy: 0.9660\n",
    "Epoch 4/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0421 - accuracy: 0.9890 - val_loss: 0.0412 - val_accuracy: 0.9860\n",
    "Epoch 5/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0357 - accuracy: 0.9910 - val_loss: 0.0315 - val_accuracy: 0.9880\n",
    "Epoch 6/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0225 - accuracy: 0.9960 - val_loss: 0.0394 - val_accuracy: 0.9880\n",
    "Epoch 7/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0144 - accuracy: 0.9970 - val_loss: 0.0339 - val_accuracy: 0.9900\n",
    "Epoch 8/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.0460 - val_accuracy: 0.9880\n",
    "Epoch 9/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0107 - accuracy: 0.9970 - val_loss: 0.0467 - val_accuracy: 0.9900\n",
    "Epoch 10/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0583 - val_accuracy: 0.9860\n",
    "Evaluating model 1...\n",
    "500/500 [==============================] - 93s 185ms/step\n",
    "Training model 2...\n",
    "Train on 1000 samples, validate on 500 samples\n",
    "Epoch 1/10\n",
    "1000/1000 [==============================] - 503s 503ms/step - loss: 0.1382 - accuracy: 0.9360 - val_loss: 0.0586 - val_accuracy: 0.9920\n",
    "Epoch 2/10\n",
    "1000/1000 [==============================] - 505s 505ms/step - loss: 0.0130 - accuracy: 0.9950 - val_loss: 0.0674 - val_accuracy: 0.9900\n",
    "Epoch 3/10\n",
    "1000/1000 [==============================] - 494s 494ms/step - loss: 0.0306 - accuracy: 0.9910 - val_loss: 0.0628 - val_accuracy: 0.9940\n",
    "Epoch 4/10\n",
    "1000/1000 [==============================] - 488s 488ms/step - loss: 0.0261 - accuracy: 0.9900 - val_loss: 0.0629 - val_accuracy: 0.9920\n",
    "Epoch 5/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0302 - accuracy: 0.9940 - val_loss: 0.0878 - val_accuracy: 0.9900\n",
    "Evaluating model 2...\n",
    "500/500 [==============================] - 93s 185ms/step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss, Test Accuracy: [0.1263430304825306, 0.9739999771118164]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Bayl3x/RaceGenderPrediction/blob/master/gender_model_loss.png?raw=true\"/>\n",
    "<img src=\"https://github.com/Bayl3x/RaceGenderPrediction/blob/master/gender_model_accuracy.png?raw=true\"/>\n",
    "<img src=\"https://github.com/Bayl3x/RaceGenderPrediction/blob/master/gender_model_cm.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5:\n",
    "Here I build and train the model for race detection. The training uses a kfold cross validation and the model with the best evaluation accuracy is selected as the final model. Again due to the limitation of my machine the number of folder, epoch, and batch size are limited to a small number (see build_model.py for parameters). An early stopping is used to prevent overfitting, it does so by monitoring the training loss and stoping after 3 epochs of unimproved loss. The stats below showed poor resuslts for latino and indian as shown in the confusion matrix below. This is likely due to the similar skin color of the two groups. Again The training loss stop decreasing after some epochs and started increasing, this is likely due to the learning rate being too large at this point and causing the gradient to not converge. Vanishing/Exploding Gradient can also cause this issue but will need more epochs to observe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # race model\n",
    "    if  model_type == 'race':\n",
    "        print('Building race model...')\n",
    "        i = 0\n",
    "        race_model = [None] * split\n",
    "        race_model_train = [None] * split\n",
    "        race_model_evaluate = [None] * split\n",
    "        \n",
    "        for train, test in kfold.split(X_race_train, y_race_train):\n",
    "            print('Training model ' + str(i) + '...')\n",
    "            race_model_output = Sequential()\n",
    "            race_model_output = Convolution2D(len(race), (1, 1), name = 'predictions')(model.layers[-4].output)\n",
    "            race_model_output = Flatten()(race_model_output)\n",
    "            race_model_output = Activation('softmax')(race_model_output)\n",
    "            \n",
    "            race_model[i] = Model(inputs = model.input, outputs = race_model_output)\n",
    "            \n",
    "            race_model[i].compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])\n",
    "            \n",
    "            race_model_train[i] = race_model[i].fit(X_race_train[train], y_race_train[train],\n",
    "                                              validation_data = (X_race_train[test], y_race_train[test]),\n",
    "                                              batch_size = len(X_race_train[train])//num_of_batches,\n",
    "                                              epochs = epoch, callbacks = [early_stop], verbose = 1)\n",
    "            \n",
    "            print('Evaluating model ' + str(i) + '...')\n",
    "            race_model_evaluate[i] = race_model[i].evaluate(X_race_test, y_race_test, batch_size = len(X_race_train[train])//num_of_batches)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        best_model = 0\n",
    "        for m in range (len(race_model_evaluate) - 1):\n",
    "            if race_model_evaluate[m][1] > race_model_evaluate[m + 1][1]:\n",
    "                best_model = m\n",
    "        \n",
    "        race_model[m].save('race_model.h5')\n",
    "        race_model[m].save_weights(\"race_model_weights.h5\")\n",
    "        \n",
    "        plt.title('Race Model Loss')\n",
    "        plt.plot(race_model_train[best_model].history['loss'], label = 'train')\n",
    "        plt.plot(race_model_train[best_model].history['val_loss'], label = 'val')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig('race_model_loss.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.title('Race Model Accuracy')\n",
    "        plt.plot(race_model_train[best_model].history['accuracy'], label = 'train')\n",
    "        plt.plot(race_model_train[best_model].history['val_accuracy'], label = 'val')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig('race_model_accuracy.png')\n",
    "        plt.show()\n",
    "\n",
    "        np.savetxt('race_model_evaluation.txt', race_model_evaluate[0])\n",
    "        print ('Test Loss, Test Accuracy:', race_model_evaluate[best_model])\n",
    "        \n",
    "        race_predicted = race_model[best_model].predict(X_race_test)\n",
    "        race_predicted_class = []\n",
    "        race_actual_class = []\n",
    "        \n",
    "        for p in range (len(race_predicted)):\n",
    "            race_predicted_class.append(race[np.argmax(race_predicted[p])])\n",
    "            race_actual_class.append(race[np.argmax(y_race_test[p])])\n",
    "        \n",
    "        cm = confusion_matrix(race_actual_class, race_predicted_class, race)\n",
    "        \n",
    "        df_cm = pd.DataFrame(cm, index = race, columns = race)\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.title('Race Confusion Matrix')\n",
    "        plt.ylabel('True')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.savefig('race_model_cm.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training history from the best model...\n",
    "Extracting data from images...\n",
    "Processing data...\n",
    "Loading face model...\n",
    "Building race model...\n",
    "Training model 0...\n",
    "Train on 1000 samples, validate on 500 samples\n",
    "Epoch 1/10\n",
    "1000/1000 [==============================] - 489s 489ms/step - loss: 1.3697 - accuracy: 0.4510 - val_loss: 0.9977 - val_accuracy: 0.5800\n",
    "Epoch 2/10\n",
    "1000/1000 [==============================] - 482s 482ms/step - loss: 0.7673 - accuracy: 0.7110 - val_loss: 0.7934 - val_accuracy: 0.7060\n",
    "Epoch 3/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.5217 - accuracy: 0.8180 - val_loss: 0.8219 - val_accuracy: 0.7000\n",
    "Epoch 4/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.3141 - accuracy: 0.8770 - val_loss: 1.0282 - val_accuracy: 0.6700\n",
    "Epoch 5/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.1495 - accuracy: 0.9460 - val_loss: 1.0424 - val_accuracy: 0.6940\n",
    "Epoch 6/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0703 - accuracy: 0.9810 - val_loss: 1.2206 - val_accuracy: 0.6940\n",
    "Epoch 7/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0423 - accuracy: 0.9880 - val_loss: 1.3755 - val_accuracy: 0.7060\n",
    "Epoch 8/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0387 - accuracy: 0.9880 - val_loss: 1.5334 - val_accuracy: 0.7000\n",
    "Epoch 9/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0358 - accuracy: 0.9890 - val_loss: 1.7024 - val_accuracy: 0.6900\n",
    "Epoch 10/10\n",
    "1000/1000 [==============================] - 482s 482ms/step - loss: 0.0503 - accuracy: 0.9780 - val_loss: 3.0088 - val_accuracy: 0.5480\n",
    "Evaluating model 0...\n",
    "500/500 [==============================] - 92s 184ms/step\n",
    "Training model 1...\n",
    "Train on 1000 samples, validate on 500 samples\n",
    "Epoch 1/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.8006 - accuracy: 0.7220 - val_loss: 0.1328 - val_accuracy: 0.9720\n",
    "Epoch 2/10\n",
    "1000/1000 [==============================] - 482s 482ms/step - loss: 0.3575 - accuracy: 0.8720 - val_loss: 0.1627 - val_accuracy: 0.9540\n",
    "Epoch 3/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.2152 - accuracy: 0.9310 - val_loss: 0.1662 - val_accuracy: 0.9400\n",
    "Epoch 4/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.1229 - accuracy: 0.9580 - val_loss: 0.1266 - val_accuracy: 0.9600\n",
    "Epoch 5/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0643 - accuracy: 0.9830 - val_loss: 0.1673 - val_accuracy: 0.9500\n",
    "Epoch 6/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0518 - accuracy: 0.9820 - val_loss: 0.1579 - val_accuracy: 0.9420\n",
    "Epoch 7/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0372 - accuracy: 0.9900 - val_loss: 0.2109 - val_accuracy: 0.9300\n",
    "Epoch 8/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0272 - accuracy: 0.9940 - val_loss: 0.2575 - val_accuracy: 0.9260\n",
    "Epoch 9/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0476 - accuracy: 0.9870 - val_loss: 0.1898 - val_accuracy: 0.9400\n",
    "Epoch 10/10\n",
    "1000/1000 [==============================] - 480s 480ms/step - loss: 0.0784 - accuracy: 0.9780 - val_loss: 0.6363 - val_accuracy: 0.8480\n",
    "Evaluating model 1...\n",
    "500/500 [==============================] - 92s 184ms/step\n",
    "Training model 2...\n",
    "Train on 1000 samples, validate on 500 samples\n",
    "Epoch 1/10\n",
    "1000/1000 [==============================] - 540s 540ms/step - loss: 0.5004 - accuracy: 0.8180 - val_loss: 0.0295 - val_accuracy: 0.9900\n",
    "Epoch 2/10\n",
    "1000/1000 [==============================] - 485s 485ms/step - loss: 0.1028 - accuracy: 0.9650 - val_loss: 0.0411 - val_accuracy: 0.9900\n",
    "Epoch 3/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0753 - accuracy: 0.9790 - val_loss: 0.0691 - val_accuracy: 0.9880\n",
    "Epoch 4/10\n",
    "1000/1000 [==============================] - 483s 483ms/step - loss: 0.0568 - accuracy: 0.9810 - val_loss: 0.0485 - val_accuracy: 0.9900\n",
    "Epoch 5/10\n",
    "1000/1000 [==============================] - 484s 484ms/step - loss: 0.0289 - accuracy: 0.9900 - val_loss: 0.0334 - val_accuracy: 0.9920\n",
    "Epoch 6/10\n",
    "1000/1000 [==============================] - 485s 485ms/step - loss: 0.0225 - accuracy: 0.9940 - val_loss: 0.0703 - val_accuracy: 0.9840\n",
    "Epoch 7/10\n",
    "1000/1000 [==============================] - 482s 482ms/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 0.0537 - val_accuracy: 0.9920\n",
    "Epoch 8/10\n",
    "1000/1000 [==============================] - 481s 481ms/step - loss: 0.0329 - accuracy: 0.9930 - val_loss: 0.0873 - val_accuracy: 0.9860\n",
    "Epoch 9/10\n",
    "1000/1000 [==============================] - 482s 482ms/step - loss: 0.0261 - accuracy: 0.9910 - val_loss: 0.0970 - val_accuracy: 0.9820\n",
    "Evaluating model 2...\n",
    "500/500 [==============================] - 92s 184ms/step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss, Test Accuracy: [2.9952571392059326, 0.5920000076293945]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Bayl3x/RaceGenderPrediction/blob/master/race_model_loss.png?raw=true\"/>\n",
    "<img src=\"https://github.com/Bayl3x/RaceGenderPrediction/blob/master/race_model_accuracy.png?raw=true\"/>\n",
    "<img src=\"https://github.com/Bayl3x/RaceGenderPrediction/blob/master/race_model_cm.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement:\n",
    "Here are a few ways to improve the model:\n",
    "1. Use a larger dataset and train with a larger number of folder, epochs, and batch_size\n",
    "2. Adjust and Add/Subtract the of layers of the Neural Nets (more research needs to be done here on how and what to adjust)\n",
    "3. Can split the training of race base on gender (one model for race_male and one model for race_female). This will help identify the most discriminating features between race only. (This is the approach that I would take if I had more compute power. Since the gender model showed good accuracy, I would first send new (unlabeled) data to the gender model as an initial filter and then send the output to either the race_female or race_male model for race prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
